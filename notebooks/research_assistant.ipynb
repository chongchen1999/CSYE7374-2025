{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Research Assistant with Retrieval Augmented Generation (RAG)\n",
    "\n",
    "This notebook demonstrates how to build a research assistant that can:\n",
    "1. Search and retrieve papers from PubMed\n",
    "2. Extract and process their content\n",
    "3. Use modern language models to answer questions based on the retrieved papers\n",
    "\n",
    "## Key Components:\n",
    "- **Paper Retrieval**: Using PubMed scraper\n",
    "- **Text Embedding**: Using BGE embeddings (state-of-the-art for scientific text)\n",
    "- **Vector Storage**: Using FAISS for efficient similarity search\n",
    "- **Language Model**: Using Mistral-7B (optimal for 4xV100 setup)\n",
    "- **RAG Pipeline**: Combining all components for intelligent question answering\n",
    "\n",
    "## Setup Requirements\n",
    "First, let's install the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing PyTorch with CUDA 12.1 support - large download due to GPU dependencies\n",
    "!pip install torch --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Installing multiple ML libraries - these are large packages with many dependencies:\n",
    "# - transformers: Hugging Face's ML model library (~500MB)\n",
    "# - bitsandbytes: For model quantization\n",
    "# - faiss-gpu: GPU-accelerated similarity search library (~200MB)\n",
    "# - sentence-transformers: For text embeddings (~100MB)\n",
    "# - vllm: For fast LLM inference\n",
    "# - einops: For tensor operations\n",
    "!pip install transformers accelerate bitsandbytes faiss-gpu sentence-transformers einops\n",
    "!pip install beautifulsoup4 pdfplumber lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n",
      "Number of GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import torch  # Deep learning framework for GPU-accelerated tensor operations\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig  # Hugging Face tools for loading and configuring language models\n",
    "from sentence_transformers import SentenceTransformer  # For generating text embeddings/vectors\n",
    "import faiss  # Fast similarity search and clustering of dense vectors\n",
    "import numpy as np  # Numerical computing library for array operations\n",
    "from typing import List, Dict  # Type hints for better code documentation\n",
    "from scrape import PaperScraper  # Our existing scraper for retrieving research papers\n",
    "\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Models\n",
    "\n",
    "We'll use:\n",
    "- BGE-Large for embeddings (optimized for scientific text)\n",
    "- Mistral-7B as our base LLM (excellent performance/resource ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Hugging Face Hub to access models\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: BAAI/bge-large-en-v1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [01:09<00:00, 34.73s/it]\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.27s/it]\n"
     ]
    }
   ],
   "source": [
    "# Initialize embedding model for generating text embeddings\n",
    "embedding_model = SentenceTransformer('BAAI/bge-large-en-v1.5')  # Load the BGE large model for high quality embeddings\n",
    "embedding_model.to('cuda')  # Move embedding model to GPU for faster inference\n",
    "\n",
    "# Configure quantization for efficient GPU usage\n",
    "bnb_config = BitsAndBytesConfig(  # Configure 4-bit quantization settings\n",
    "    load_in_4bit=True,  # Enable 4-bit quantization for reduced memory usage\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Use normalized float4 quantization for better accuracy\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # Use float16 for compute to balance speed and precision\n",
    "    bnb_4bit_use_double_quant=True  # Enable double quantization for additional memory savings\n",
    ")\n",
    "\n",
    "# Initialize LLM and tokenizer\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"  # Specify the Mistral model to use\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)  # Load tokenizer for converting text to tokens\n",
    "model = AutoModelForCausalLM.from_pretrained(  # Load the language model\n",
    "    model_name,  # Use the specified Mistral model\n",
    "    quantization_config=bnb_config,  # Apply the quantization settings\n",
    "    torch_dtype=torch.float16,  # Use float16 for model weights\n",
    "    device_map=\"auto\",  # Automatically distribute model across available GPUs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Research Assistant Class\n",
    "\n",
    "This class combines paper retrieval, embedding, and question answering capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchAssistant:\n",
    "    def __init__(self):\n",
    "        # Initialize components for paper processing and analysis\n",
    "        self.scraper = PaperScraper()  # For retrieving papers from PubMed\n",
    "        self.embedding_model = embedding_model  # For generating text embeddings\n",
    "        self.tokenizer = tokenizer  # For tokenizing text for the LLM\n",
    "        self.model = model  # The language model for answering questions\n",
    "        self.paper_texts = []  # Store the full text of processed papers\n",
    "        self.paper_metadata = []  # Store metadata (title, authors, etc) for papers\n",
    "        self.index = None  # Will hold the FAISS similarity search index\n",
    "        \n",
    "    def search_papers(self, query: str, max_results: int = 10):\n",
    "        \"\"\"Search and download papers for the given query\"\"\"\n",
    "        print(f\"Searching for papers about: {query}\")\n",
    "        \n",
    "        # Get paper IDs from PubMed search\n",
    "        pmids = self.scraper.search_pubmed(query, max_results)\n",
    "        # Fetch detailed information for each paper\n",
    "        papers = self.scraper.fetch_pubmed_details(pmids)\n",
    "        \n",
    "        # Process each paper found\n",
    "        for paper in papers:\n",
    "            if pdf_url := paper.get('full_text_link'):  # Check if full text PDF is available\n",
    "                try:\n",
    "                    # Download PDF to temporary file\n",
    "                    pdf_path = self.scraper.download_pdf(\n",
    "                        pdf_url, \n",
    "                        f\"temp_{paper['pubmed_id']}.pdf\"\n",
    "                    )\n",
    "                    # Extract plain text from PDF\n",
    "                    text = self.scraper.extract_text_from_pdf(pdf_path)\n",
    "                    \n",
    "                    # Save paper content and metadata\n",
    "                    self.paper_texts.append(text)\n",
    "                    self.paper_metadata.append(paper)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing paper {paper['pubmed_id']}: {e}\")\n",
    "                    \n",
    "        self._build_index()  # Create search index from processed papers\n",
    "        print(f\"Successfully processed {len(self.paper_texts)} papers\")\n",
    "    \n",
    "    def _build_index(self):\n",
    "        \"\"\"Create FAISS index from paper embeddings\"\"\"\n",
    "        # Initialize lists for storing chunks and their metadata\n",
    "        self.chunks = []\n",
    "        self.chunk_metadata = []\n",
    "        \n",
    "        # Process each paper into chunks\n",
    "        for text, metadata in zip(self.paper_texts, self.paper_metadata):\n",
    "            # Split text into paragraphs\n",
    "            paragraphs = text.split('\\n\\n')\n",
    "            # Create overlapping chunks of 3 paragraphs\n",
    "            for i in range(0, len(paragraphs), 2):\n",
    "                chunk = ' '.join(paragraphs[i:i+2])\n",
    "                if len(chunk.split()) > 30:  # Only keep chunks with sufficient content\n",
    "                    self.chunks.append(chunk)\n",
    "                    self.chunk_metadata.append(metadata)\n",
    "        \n",
    "        # Generate embeddings for all chunks\n",
    "        embeddings = self.embedding_model.encode(\n",
    "            self.chunks,\n",
    "            batch_size=4,  # Process 4 chunks at a time\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True  # Convert to numpy for FAISS compatibility\n",
    "        )\n",
    "        \n",
    "        # Initialize and populate FAISS index\n",
    "        dimension = embeddings.shape[1]  # Get embedding dimension\n",
    "        self.index = faiss.IndexFlatL2(dimension)  # Create L2 distance index\n",
    "        self.index.add(embeddings)  # Add embeddings to index\n",
    "    \n",
    "    def answer_question(self, question: str, k: int = 5):\n",
    "        \"\"\"Answer a question using RAG\"\"\"\n",
    "        # Convert question to embedding vector\n",
    "        q_embedding = self.embedding_model.encode([question])[0]\n",
    "        \n",
    "        # Find k most similar chunks\n",
    "        distances, indices = self.index.search(q_embedding.reshape(1, -1), k)\n",
    "        \n",
    "        # Build context from relevant chunks - LIMIT TOTAL LENGTH\n",
    "        context = \"\"\n",
    "        used_papers = set()\n",
    "        total_tokens = 0\n",
    "        max_tokens = 2048  # Set a reasonable limit for context length\n",
    "        \n",
    "        for idx in indices[0]:\n",
    "            chunk = self.chunks[idx]\n",
    "            metadata = self.chunk_metadata[idx]\n",
    "            paper_id = metadata['pubmed_id']\n",
    "            \n",
    "            # Only include first chunk from each paper and check token length\n",
    "            if paper_id not in used_papers:\n",
    "                chunk_tokens = len(self.tokenizer.encode(chunk))\n",
    "                if total_tokens + chunk_tokens > max_tokens:\n",
    "                    break\n",
    "                    \n",
    "                used_papers.add(paper_id)\n",
    "                context += f\"\\nFrom paper '{metadata['title']}':\\n{chunk}\\n\"\n",
    "                total_tokens += chunk_tokens\n",
    "        \n",
    "        # Construct shorter prompt\n",
    "        prompt = f\"\"\"Answer based on these excerpts. Include citations.\n",
    "\n",
    "        Excerpts: {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer: \"\"\"\n",
    "            \n",
    "        # Generate answer using LLM with controlled length\n",
    "        inputs = self.tokenizer(\n",
    "            prompt, \n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=2048  # Hard limit on input length\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,  # Limit response length\n",
    "            temperature=0.7,  # Add some randomness to generation\n",
    "            num_return_sequences=1,  # Generate one response\n",
    "            do_sample=True,  # Use sampling instead of greedy decoding\n",
    "        )\n",
    "        \n",
    "        # Extract and clean up the generated answer\n",
    "        answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return answer.split(\"Answer: \")[-1].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Example Usage\n",
    "\n",
    "Let's demonstrate how to use the Research Assistant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:scrape:Searching PubMed for : latest developments in CRISPR gene editing cancer therapy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for papers about: latest developments in CRISPR gene editing cancer therapy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:scrape:Found 27 results\n",
      "INFO:scrape:Fetching details for 27 papers...\n",
      "INFO:scrape:Successfully processed PMID 37356052\n",
      "INFO:scrape:Successfully processed PMID 36610813\n",
      "ERROR:scrape:Error processing PMID 36272261: 429 Client Error: Too Many Requests for url: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=36272261&retmode=xml&rettype=full\n",
      "INFO:scrape:Successfully processed PMID 35337340\n",
      "ERROR:scrape:Error processing PMID 39708520: 429 Client Error: Too Many Requests for url: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=39708520&retmode=xml&rettype=full\n",
      "INFO:scrape:Successfully processed PMID 38050977\n",
      "INFO:scrape:Successfully processed PMID 34411650\n",
      "INFO:scrape:Successfully processed PMID 31739699\n",
      "INFO:scrape:Successfully processed PMID 36560658\n",
      "INFO:scrape:Successfully processed PMID 33003295\n",
      "INFO:scrape:Successfully processed PMID 39317648\n",
      "INFO:scrape:Successfully processed PMID 35547744\n",
      "INFO:scrape:Successfully processed PMID 39292321\n",
      "INFO:scrape:Successfully processed PMID 35999480\n",
      "INFO:scrape:Successfully processed PMID 38041049\n",
      "INFO:scrape:Successfully processed PMID 32264803\n",
      "INFO:scrape:Successfully processed PMID 36139078\n",
      "INFO:scrape:Successfully processed PMID 29691470\n",
      "INFO:scrape:Successfully processed PMID 35358798\n",
      "INFO:scrape:Successfully processed PMID 34713248\n",
      "INFO:scrape:Successfully processed PMID 33371215\n",
      "INFO:scrape:Successfully processed PMID 38310456\n",
      "INFO:scrape:Successfully processed PMID 37545273\n",
      "INFO:scrape:Successfully processed PMID 33213345\n",
      "INFO:scrape:Successfully processed PMID 39459899\n",
      "INFO:scrape:Successfully processed PMID 37451978\n",
      "INFO:scrape:Successfully processed PMID 30194069\n",
      "INFO:scrape:Successfully fetched details for 25 papers\n",
      "INFO:scrape:Downloading PDF from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10477906/pdf\n",
      "INFO:scrape:PDF saved to temp_37356052.pdf\n",
      "INFO:scrape:Extracting text from temp_37356052.pdf\n",
      "INFO:scrape:Successfully extracted text from temp_37356052.pdf\n",
      "INFO:scrape:Downloading PDF from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8953071/pdf\n",
      "INFO:scrape:PDF saved to temp_35337340.pdf\n",
      "INFO:scrape:Extracting text from temp_35337340.pdf\n",
      "INFO:scrape:Successfully extracted text from temp_35337340.pdf\n",
      "INFO:scrape:Downloading PDF from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9787400/pdf\n",
      "INFO:scrape:PDF saved to temp_36560658.pdf\n",
      "INFO:scrape:Extracting text from temp_36560658.pdf\n",
      "INFO:scrape:Successfully extracted text from temp_36560658.pdf\n",
      "INFO:scrape:Downloading PDF from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7599677/pdf\n",
      "INFO:scrape:PDF saved to temp_33003295.pdf\n",
      "INFO:scrape:Extracting text from temp_33003295.pdf\n",
      "INFO:scrape:Successfully extracted text from temp_33003295.pdf\n",
      "INFO:scrape:Downloading PDF from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9065202/pdf\n",
      "INFO:scrape:PDF saved to temp_35547744.pdf\n",
      "INFO:scrape:Extracting text from temp_35547744.pdf\n",
      "INFO:scrape:Successfully extracted text from temp_35547744.pdf\n",
      "INFO:scrape:Downloading PDF from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3307543/pdf\n",
      "INFO:scrape:PDF saved to temp_39292321.pdf\n",
      "INFO:scrape:Extracting text from temp_39292321.pdf\n",
      "INFO:scrape:Successfully extracted text from temp_39292321.pdf\n",
      "INFO:scrape:Downloading PDF from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10693129/pdf\n",
      "INFO:scrape:PDF saved to temp_38041049.pdf\n",
      "INFO:scrape:Extracting text from temp_38041049.pdf\n",
      "INFO:scrape:Successfully extracted text from temp_38041049.pdf\n",
      "INFO:scrape:Downloading PDF from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9496048/pdf\n",
      "INFO:scrape:PDF saved to temp_36139078.pdf\n",
      "INFO:scrape:Extracting text from temp_36139078.pdf\n",
      "INFO:scrape:Successfully extracted text from temp_36139078.pdf\n",
      "INFO:scrape:Downloading PDF from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8525358/pdf\n",
      "INFO:scrape:PDF saved to temp_34713248.pdf\n",
      "INFO:scrape:Extracting text from temp_34713248.pdf\n",
      "INFO:scrape:Successfully extracted text from temp_34713248.pdf\n",
      "INFO:scrape:Downloading PDF from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7766694/pdf\n",
      "INFO:scrape:PDF saved to temp_33371215.pdf\n",
      "INFO:scrape:Extracting text from temp_33371215.pdf\n",
      "INFO:scrape:Successfully extracted text from temp_33371215.pdf\n",
      "INFO:scrape:Downloading PDF from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11512240/pdf\n",
      "INFO:scrape:PDF saved to temp_39459899.pdf\n",
      "INFO:scrape:Extracting text from temp_39459899.pdf\n",
      "INFO:scrape:Successfully extracted text from temp_39459899.pdf\n",
      "INFO:scrape:Downloading PDF from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10935483/pdf\n",
      "INFO:scrape:PDF saved to temp_37451978.pdf\n",
      "INFO:scrape:Extracting text from temp_37451978.pdf\n",
      "INFO:scrape:Successfully extracted text from temp_37451978.pdf\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 12 papers\n",
      "\n",
      "Q: What are the main challenges in using CRISPR for cancer therapy?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 55.03it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A: CRISPR-Cas9 is a powerful tool for precise genome editing, but its application in cancer therapy faces several challenges. One of the main challenges is the difficulty in targeting cancer cells without affecting healthy cells. This is because cancer cells often have mutations that are also present in healthy cells. The specificity of CRISPR-Cas9 depends on the guide RNA (gRNA) used, which must be designed to target the specific mutation present in the cancer cells. However, designing such gRNAs can be challenging, especially when the mutation is present in a highly conserved region of the genome.\n",
      "\n",
      "Another challenge is the potential for off-target effects, where the gRNA binds to a different DNA sequence than intended, leading to unintended consequences. Off-target effects can occur when the gRNA binds to a non-target sequence with similarity to the intended target. This can lead to the deletion or insertion of nucleotides in the genome, which can have deleterious effects on the cell. Off-target effects can be reduced by optimizing the design of the gRNA and by using multiple gRNAs to target\n"
     ]
    }
   ],
   "source": [
    "# Initialize the assistant\n",
    "assistant = ResearchAssistant()\n",
    "\n",
    "# Search for papers on a topic\n",
    "assistant.search_papers(\n",
    "    query=\"latest developments in CRISPR gene editing cancer therapy\",\n",
    "    max_results=20\n",
    ")\n",
    "\n",
    "# Ask questions\n",
    "questions = [\n",
    "    \"What are the main challenges in using CRISPR for cancer therapy?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(f\"\\nA: {assistant.answer_question(question)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Components\n",
    "\n",
    "1. **Paper Retrieval**:\n",
    "   - Uses PubMed API to search for relevant papers\n",
    "   - Downloads PDFs and extracts text\n",
    "\n",
    "2. **Text Processing**:\n",
    "   - Splits papers into manageable chunks\n",
    "   - Maintains metadata for citations\n",
    "\n",
    "3. **Embedding & Indexing**:\n",
    "   - Uses BGE-Large embeddings (state-of-the-art for scientific text)\n",
    "   - FAISS for efficient similarity search\n",
    "\n",
    "4. **Language Model**:\n",
    "   - Mistral-7B with 4-bit quantization\n",
    "   - Optimized for multi-GPU inference\n",
    "\n",
    "5. **RAG Process**:\n",
    "   - Embeds user question\n",
    "   - Retrieves relevant context\n",
    "   - Generates contextualized answer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
