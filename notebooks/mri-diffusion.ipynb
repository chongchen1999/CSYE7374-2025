{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: All necessary imports\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "                                               \n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIT1T2Dataset(Dataset):\n",
    "    \"\"\"Dataset class for loading paired/unpaired T1-T2 MRI data.\n",
    "    Handles loading, validation, normalization and caching of MRI volumes.\"\"\"\n",
    "    \n",
    "    def __init__(self, t1_dir, t2_dir, slice_mode='middle', paired=True, transform=None, cache_size=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            t1_dir (str): Directory containing T1 scans\n",
    "            t2_dir (str): Directory containing T2 scans\n",
    "            slice_mode (str): 'middle' or 'random' - how to select slice from volume\n",
    "            paired (bool): If True, uses paired T1-T2 data, else random unpaired selection\n",
    "            transform: Optional transforms to apply to slices\n",
    "            cache_size (int): Number of volumes to cache in memory (0 for no caching)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Add resize transform\n",
    "        self.resize_transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((128, 128)), \n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        # Store initialization parameters\n",
    "        self.t1_dir = t1_dir\n",
    "        self.t2_dir = t2_dir\n",
    "        self.transform = transform\n",
    "        self.slice_mode = slice_mode\n",
    "        self.paired = paired\n",
    "        self.cache_size = cache_size\n",
    "        \n",
    "        # Get lists of all NIfTI files (.nii.gz)\n",
    "        self.t1_files = sorted([f for f in os.listdir(t1_dir) if f.endswith('.nii.gz')])\n",
    "        self.t2_files = sorted([f for f in os.listdir(t2_dir) if f.endswith('.nii.gz')])\n",
    "        \n",
    "        # For paired training, find matching T1-T2 pairs based on subject ID\n",
    "        if self.paired:\n",
    "            self.paired_files = []\n",
    "            for t1f in self.t1_files:\n",
    "                subject_id = t1f.split('-')[0][3:]  # Extract subject ID from filename\n",
    "                matching_t2 = [t2f for t2f in self.t2_files if t2f.split('-')[0][3:] == subject_id]\n",
    "                if matching_t2:\n",
    "                    self.paired_files.append((t1f, matching_t2[0]))\n",
    "            print(f\"Found {len(self.paired_files)} paired T1/T2 datasets\")\n",
    "            self.data_files = self.paired_files\n",
    "        else:\n",
    "            # For unpaired, just use T1 files and randomly select T2 later\n",
    "            self.data_files = [(t1f, None) for t1f in self.t1_files]\n",
    "        \n",
    "        # Initialize cache dictionary\n",
    "        self.cache = {}\n",
    "\n",
    "    def _load_and_validate_volume(self, filename, is_t1=True):\n",
    "        \"\"\"Load and validate a single MRI volume.\n",
    "        \n",
    "        Args:\n",
    "            filename (str): Name of the NIfTI file\n",
    "            is_t1 (bool): Whether this is a T1 volume (determines directory)\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (volume data array, (min_value, max_value))\n",
    "        \"\"\"\n",
    "        dir_path = self.t1_dir if is_t1 else self.t2_dir\n",
    "        filepath = os.path.join(dir_path, filename)\n",
    "        \n",
    "        # Check if volume is in cache\n",
    "        if filepath in self.cache:\n",
    "            vol = self.cache[filepath]\n",
    "        else:\n",
    "            # Load volume using NiBabel\n",
    "            vol = nib.load(filepath).get_fdata()\n",
    "            \n",
    "            # Validate volume\n",
    "            if not self.is_valid_volume(vol):\n",
    "                raise ValueError(f\"Invalid volume: {filename}\")\n",
    "            \n",
    "            # Cache volume if cache isn't full\n",
    "            if len(self.cache) < self.cache_size:\n",
    "                self.cache[filepath] = vol\n",
    "    \n",
    "        # Calculate volume statistics for normalization\n",
    "        stats = (float(vol.min()), float(vol.max()))\n",
    "        \n",
    "        return vol, stats\n",
    "\n",
    "    def is_valid_volume(self, vol):\n",
    "        \"\"\"Check if volume meets quality criteria.\n",
    "        \n",
    "        Args:\n",
    "            vol (np.ndarray): Volume data\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if volume is valid\n",
    "        \"\"\"\n",
    "        min_size = 64  # Minimum dimension size\n",
    "        return (vol.shape[0] >= min_size and \n",
    "                vol.shape[1] >= min_size and \n",
    "                vol.shape[2] >= 1 and \n",
    "                not np.any(np.isnan(vol)))\n",
    "\n",
    "    def get_slice_idx(self, volume):\n",
    "        \"\"\"Get slice index based on slice_mode setting.\n",
    "        \n",
    "        Args:\n",
    "            volume (np.ndarray): Volume data\n",
    "            \n",
    "        Returns:\n",
    "            int: Index of slice to extract\n",
    "        \"\"\"\n",
    "        if self.slice_mode == 'middle':\n",
    "            return volume.shape[2] // 2\n",
    "        else:  # random\n",
    "            return random.randint(0, volume.shape[2] - 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of T1-T2 pairs in the dataset.\"\"\"\n",
    "        return len(self.data_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get a T1-T2 pair of slices.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the pair\n",
    "            \n",
    "        Returns:\n",
    "            dict: Contains 'T1' and 'T2' tensor slices\n",
    "        \"\"\"\n",
    "        t1_file, t2_file = self.data_files[idx]\n",
    "        \n",
    "        # Load T1 volume and get stats\n",
    "        t1_vol, t1_stats = self._load_and_validate_volume(t1_file, is_t1=True)\n",
    "        \n",
    "        if self.paired:\n",
    "            # Load matching T2 volume\n",
    "            t2_vol, t2_stats = self._load_and_validate_volume(t2_file, is_t1=False)\n",
    "        else:\n",
    "            # Random T2 volume for unpaired\n",
    "            random_t2_idx = random.randint(0, len(self.t2_files) - 1)\n",
    "            t2_vol, t2_stats = self._load_and_validate_volume(\n",
    "                self.t2_files[random_t2_idx], is_t1=False\n",
    "            )\n",
    "            \n",
    "        # Get slice indices\n",
    "        t1_slice_idx = self.get_slice_idx(t1_vol)\n",
    "        t2_slice_idx = (t1_slice_idx if self.paired \n",
    "                       else self.get_slice_idx(t2_vol))\n",
    "\n",
    "        # Extract and normalize slices\n",
    "        t1_slice = self.normalize_slice(t1_vol[:,:,t1_slice_idx], t1_stats)\n",
    "        t2_slice = self.normalize_slice(t2_vol[:,:,t2_slice_idx], t2_stats)\n",
    "\n",
    "        # Convert to tensors and add channel dimension\n",
    "        t1_tensor = torch.from_numpy(t1_slice).float().unsqueeze(0)\n",
    "        t2_tensor = torch.from_numpy(t2_slice).float().unsqueeze(0)\n",
    "\n",
    "        # Apply resize transform\n",
    "        t1_tensor = self.resize_transform(t1_tensor)\n",
    "        t2_tensor = self.resize_transform(t2_tensor)\n",
    "\n",
    "        # Apply any additional transforms\n",
    "        if self.transform:\n",
    "            t1_tensor = self.transform(t1_tensor)\n",
    "            t2_tensor = self.transform(t2_tensor)\n",
    "\n",
    "        # Clear from memory if not cached\n",
    "        if t1_file not in self.cache:\n",
    "            del t1_vol\n",
    "        if t2_file not in self.cache:\n",
    "            del t2_vol\n",
    "\n",
    "        return {'T1': t1_tensor, 'T2': t2_tensor}\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_slice(slice_data, stats):\n",
    "        \"\"\"Normalize slice to [0,1] range using pre-computed statistics.\n",
    "        \n",
    "        Args:\n",
    "            slice_data (np.ndarray): Raw slice data\n",
    "            stats (tuple): (min_value, max_value) for normalization\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Normalized slice\n",
    "        \"\"\"\n",
    "        min_val, max_val = stats\n",
    "        return (slice_data - min_val) / (max_val - min_val + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model (U-net with Time + Cross-attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    \"\"\"Time embedding module that projects timesteps into a higher dimensional space.\n",
    "    This is crucial for conditioning the diffusion model on the noise level/timestep.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_channels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_channels (int): Number of channels in the embedding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        # Project scalar timestep to higher dimension and add non-linearity\n",
    "        self.time_proj = nn.Sequential(\n",
    "            nn.Linear(1, n_channels),\n",
    "            nn.SiLU(),  # Sigmoid Linear Unit activation\n",
    "            nn.Linear(n_channels, n_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        \"\"\"Projects timesteps into the embedding space.\n",
    "        \n",
    "        Args:\n",
    "            t (torch.Tensor): Batch of timesteps [batch_size]\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Time embeddings [batch_size, n_channels]\n",
    "        \"\"\"\n",
    "        t = t.unsqueeze(-1).float()  # Add feature dimension\n",
    "        return self.time_proj(t)\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Convolutional block with time conditioning and residual connections.\n",
    "    This is the basic building block of the UNet architecture.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, time_channels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels\n",
    "            out_channels (int): Number of output channels\n",
    "            time_channels (int): Number of channels in time embedding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Main convolution path\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.norm1 = nn.GroupNorm(8, out_channels)  # Normalization for stability\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "        self.norm2 = nn.GroupNorm(8, out_channels)\n",
    "        \n",
    "        # Time conditioning projection\n",
    "        self.time_mlp = nn.Linear(time_channels, out_channels)\n",
    "        \n",
    "        # Residual connection handling\n",
    "        self.use_residual = in_channels == out_channels\n",
    "        if not self.use_residual:\n",
    "            self.residual_conv = nn.Conv2d(in_channels, out_channels, 1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input feature maps [B, C, H, W]\n",
    "            t (torch.Tensor): Time embeddings [B, time_channels]\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Processed feature maps [B, out_channels, H, W]\n",
    "        \"\"\"\n",
    "        # Save residual connection\n",
    "        residual = x if self.use_residual else self.residual_conv(x)\n",
    "        \n",
    "        # Main convolution path\n",
    "        h = self.conv1(x)\n",
    "        h = self.norm1(h)\n",
    "        # Add time conditioning\n",
    "        h += self.time_mlp(t)[:, :, None, None]  # Broadcasting time features\n",
    "        h = F.silu(h)\n",
    "        h = self.conv2(h)\n",
    "        h = self.norm2(h)\n",
    "        h = F.silu(h)\n",
    "        \n",
    "        return h + residual  # Add residual connection\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"Self-attention module for capturing long-range dependencies in the feature maps.\"\"\"\n",
    "    \n",
    "    def __init__(self, channels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            channels (int): Number of input/output channels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.mha = nn.MultiheadAttention(channels, 4, batch_first=True)\n",
    "        self.ln = nn.LayerNorm([channels])\n",
    "        self.ff_self = nn.Sequential(\n",
    "            nn.LayerNorm([channels]),\n",
    "            nn.Linear(channels, channels),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(channels, channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input feature maps [B, C, H, W]\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Self-attended feature maps [B, C, H, W]\n",
    "        \"\"\"\n",
    "        size = x.shape[-2:]  # Save spatial dimensions\n",
    "        # Reshape for attention\n",
    "        x = x.flatten(2).transpose(1, 2)  # [B, HW, C]\n",
    "        x = self.ln(x)\n",
    "        # Self attention\n",
    "        attention_value, _ = self.mha(x, x, x)\n",
    "        attention_value = attention_value + x  # Skip connection\n",
    "        # Feedforward network\n",
    "        attention_value = self.ff_self(attention_value) + attention_value\n",
    "        # Restore spatial dimensions\n",
    "        return attention_value.transpose(1, 2).view(-1, self.channels, *size)\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    \"\"\"Cross-attention module for attending between source and target features.\"\"\"\n",
    "    \n",
    "    def __init__(self, channels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            channels (int): Number of input/output channels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.mha = nn.MultiheadAttention(channels, 4, batch_first=True)\n",
    "        self.ln = nn.LayerNorm([channels])\n",
    "        self.ff_cross = nn.Sequential(\n",
    "            nn.LayerNorm([channels]),\n",
    "            nn.Linear(channels, channels),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(channels, channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, context):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Query feature maps [B, C, H, W]\n",
    "            context (torch.Tensor): Key/Value feature maps [B, C, H, W]\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Cross-attended feature maps [B, C, H, W]\n",
    "        \"\"\"\n",
    "        size = x.shape[-2:]\n",
    "        # Reshape for attention\n",
    "        x = x.flatten(2).transpose(1, 2)  # [B, HW, C]\n",
    "        context = context.flatten(2).transpose(1, 2)\n",
    "        x = self.ln(x)\n",
    "        # Cross attention\n",
    "        attention_value, _ = self.mha(x, context, context)\n",
    "        attention_value = attention_value + x  # Skip connection\n",
    "        # Feedforward network\n",
    "        attention_value = self.ff_cross(attention_value) + attention_value\n",
    "        # Restore spatial dimensions\n",
    "        return attention_value.transpose(1, 2).view(-1, self.channels, *size)\n",
    "\n",
    "# UNet Architecture\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"U-Net architecture with attention and time conditioning for diffusion models.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels=3, time_channels=256, n_channels=64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels (default: 3)\n",
    "            time_channels (int): Dimension of time embedding (default: 256)\n",
    "            n_channels (int): Base number of channels (default: 64)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Add context projection layer\n",
    "        self.context_proj = nn.Conv2d(1, n_channels * 8, kernel_size=1)\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_embed = TimeEmbedding(time_channels)\n",
    "        \n",
    "        # Encoder path\n",
    "        self.inc = ConvBlock(in_channels, n_channels, time_channels)\n",
    "        self.down1 = nn.ModuleList([\n",
    "            nn.MaxPool2d(2),\n",
    "            ConvBlock(n_channels, n_channels*2, time_channels),\n",
    "            SelfAttention(n_channels*2)\n",
    "        ])\n",
    "        self.down2 = nn.ModuleList([\n",
    "            nn.MaxPool2d(2),\n",
    "            ConvBlock(n_channels*2, n_channels*4, time_channels),\n",
    "            SelfAttention(n_channels*4)\n",
    "        ])\n",
    "        self.down3 = nn.ModuleList([\n",
    "            nn.MaxPool2d(2),\n",
    "            ConvBlock(n_channels*4, n_channels*8, time_channels),\n",
    "            SelfAttention(n_channels*8)\n",
    "        ])\n",
    "\n",
    "        # Bottleneck with attention\n",
    "        self.bot1 = ConvBlock(n_channels*8, n_channels*8, time_channels)\n",
    "        self.bot_attn = SelfAttention(n_channels*8)\n",
    "        self.cross_attn = CrossAttention(n_channels*8)\n",
    "        self.bot2 = ConvBlock(n_channels*8, n_channels*8, time_channels)\n",
    "        self.bot3 = ConvBlock(n_channels*8, n_channels*8, time_channels)\n",
    "\n",
    "        # Decoder path with skip connections\n",
    "        # For upsampling, adjust the channel dimensions to match the concatenated skip connections.\n",
    "        self.up1 = nn.ModuleList([\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            ConvBlock(n_channels*12, n_channels*4, time_channels),  # e.g., concatenating x4 (n_channels*8) and x3 (n_channels*4) gives n_channels*12\n",
    "            SelfAttention(n_channels*4)\n",
    "        ])\n",
    "        self.up2 = nn.ModuleList([\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            ConvBlock(n_channels*6, n_channels*2, time_channels),   # e.g., concatenating previous output (n_channels*4) with x2 (n_channels*2)\n",
    "            SelfAttention(n_channels*2)\n",
    "        ])\n",
    "        self.up3 = nn.ModuleList([\n",
    "            ConvBlock(n_channels*3, n_channels, time_channels)        # e.g., concatenating previous output (n_channels*2) with x1 (n_channels)\n",
    "        ])\n",
    "        \n",
    "        # Output convolution\n",
    "        self.outc = nn.Conv2d(n_channels, 1, 1)\n",
    "\n",
    "    def forward(self, x, t, condition=None, context=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor [B, C, H, W]\n",
    "            t (torch.Tensor): Timesteps [B]\n",
    "            condition (torch.Tensor, optional): Conditioning flag\n",
    "            context (torch.Tensor, optional): Context image for cross-attention [B, 1, H, W]\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Predicted noise or image delta [B, 1, H, W]\n",
    "        \"\"\"\n",
    "        # Add conditioning information if provided\n",
    "        if condition is not None:\n",
    "            condition = condition.expand(-1, 1, x.shape[2], x.shape[3])\n",
    "            x = torch.cat([x, condition], dim=1)\n",
    "            \n",
    "        # Time embedding\n",
    "        t = self.time_embed(t)\n",
    "        \n",
    "        # Encoder path with skip connections\n",
    "        x1 = self.inc(x, t)\n",
    "        x2 = self.down1[0](x1)  # MaxPool2d\n",
    "        x2 = self.down1[1](x2, t)  # ConvBlock takes `t`\n",
    "        x2 = self.down1[2](x2)  # SelfAttention\n",
    "        \n",
    "        x3 = self.down2[0](x2)  # MaxPool2d\n",
    "        x3 = self.down2[1](x3, t)  # ConvBlock takes `t`\n",
    "        x3 = self.down2[2](x3)  # SelfAttention\n",
    "\n",
    "        x4 = self.down3[0](x3)  # MaxPool2d\n",
    "        x4 = self.down3[1](x4, t)  # ConvBlock takes `t`\n",
    "        x4 = self.down3[2](x4)  # SelfAttention\n",
    "        \n",
    "        # Bottleneck with attention\n",
    "        x4 = self.bot1(x4, t)\n",
    "        x4 = self.bot_attn(x4)\n",
    "        if context is not None:\n",
    "            # Project context to match bottleneck dimensions\n",
    "            context = self.context_proj(context)  # [B, n_channels*8, H, W]\n",
    "            x4 = self.cross_attn(x4, context)\n",
    "        x4 = self.bot2(x4, t)\n",
    "        x4 = self.bot3(x4, t)\n",
    "        \n",
    "        # Decoder path with skip connections\n",
    "        # Upsample x4 to match x3\n",
    "        x4 = F.interpolate(x4, size=x3.shape[-2:], mode='bilinear', align_corners=True)\n",
    "        # Concatenate bottleneck output (upsampled x4) with the output from down2 (x3)\n",
    "        x = torch.cat([x4, x3], dim=1)\n",
    "        x = self.up1[0](x)          # Upsample (if further upsampling is needed)\n",
    "        x = self.up1[1](x, t)         # ConvBlock with time conditioning\n",
    "        x = self.up1[2](x)          # SelfAttention\n",
    "\n",
    "        # Up Block 2: Concatenate result with output from down1 (x2)\n",
    "        x = torch.cat([x, x2], dim=1)\n",
    "        x = self.up2[0](x)          # Upsample\n",
    "        x = self.up2[1](x, t)         # ConvBlock with time conditioning\n",
    "        x = self.up2[2](x)          # SelfAttention\n",
    "\n",
    "        # Up Block 3: Concatenate result with initial encoder output (x1)\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = self.up3[0](x, t)         # ConvBlock with time conditioning\n",
    "        \n",
    "        return self.outc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPMTrainer:\n",
    "    \"\"\"Denoising Diffusion Probabilistic Models (DDPM) Trainer.\n",
    "    Handles the training process, including:\n",
    "    - Forward/reverse diffusion processes\n",
    "    - Optimization\n",
    "    - Mixed precision training\n",
    "    - Sampling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, model, n_timesteps=1000, beta_start=1e-4, beta_end=0.02,\n",
    "        lr=1e-4, device=\"cuda\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: UNet model instance\n",
    "            n_timesteps (int): Number of diffusion timesteps\n",
    "            beta_start (float): Starting noise schedule value\n",
    "            beta_end (float): Ending noise schedule value\n",
    "            lr (float): Learning rate for Adam optimizer\n",
    "            device (str): Device to run on (\"cuda\" or \"cpu\")\n",
    "        \"\"\"\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.n_timesteps = n_timesteps\n",
    "        \n",
    "        # Setup noise schedule\n",
    "        self.betas = torch.linspace(beta_start, beta_end, n_timesteps).to(device)\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        \n",
    "        # Pre-compute values for diffusion process\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - self.alphas_cumprod)\n",
    "        \n",
    "        # Setup optimizer\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def diffuse_step(self, x_0, t):\n",
    "        \"\"\"Forward diffusion step: adds noise to image according to timestep.\n",
    "        \n",
    "        Args:\n",
    "            x_0 (torch.Tensor): Original clean image\n",
    "            t (torch.Tensor): Timesteps for batch\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (noisy image, noise added)\n",
    "        \"\"\"\n",
    "        noise = torch.randn_like(x_0)  # Random noise\n",
    "        \n",
    "        # Get noise scaling factors for timestep t\n",
    "        sqrt_alpha_t = self.sqrt_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha_t = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1, 1)\n",
    "        \n",
    "        # Apply forward diffusion equation\n",
    "        x_t = sqrt_alpha_t * x_0 + sqrt_one_minus_alpha_t * noise\n",
    "        return x_t, noise\n",
    "    \n",
    "    def train_one_batch(self, x_0, condition=None, context=None):\n",
    "        \"\"\"Trains model on a single batch.\n",
    "        \n",
    "        Args:\n",
    "            x_0 (torch.Tensor): Clean images [B, C, H, W]\n",
    "            condition (torch.Tensor, optional): Conditioning information\n",
    "            context (torch.Tensor, optional): Context for cross-attention\n",
    "            \n",
    "        Returns:\n",
    "            float: Batch loss value\n",
    "        \"\"\"\n",
    "        batch_size = x_0.shape[0]\n",
    "        # Sample random timesteps for batch\n",
    "        t = torch.randint(0, self.n_timesteps, (batch_size,), device=self.device)\n",
    "        \n",
    "        # Apply forward diffusion\n",
    "        x_t, noise = self.diffuse_step(x_0, t)\n",
    "\n",
    "        # Forward pass (removed autocast since it's not supported on MPS)\n",
    "        noise_pred = self.model(x_t, t, condition=condition, context=context)\n",
    "        # Calculate loss between predicted and actual noise\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        \n",
    "        # Optimizer step\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample(self, condition=None, context=None, shape=None, n_steps=None):\n",
    "        \"\"\"Generates samples using the reverse diffusion process.\n",
    "        \n",
    "        Args:\n",
    "            condition (torch.Tensor, optional): Conditioning information\n",
    "            context (torch.Tensor, optional): Context for cross-attention\n",
    "            shape (tuple): Shape of samples to generate\n",
    "            n_steps (int, optional): Number of sampling steps\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Generated samples\n",
    "        \"\"\"\n",
    "        if n_steps is None:\n",
    "            n_steps = self.n_timesteps\n",
    "        \n",
    "        # Start from pure noise\n",
    "        x_t = torch.randn(shape, device=self.device)\n",
    "        \n",
    "        # Gradually denoise the sample\n",
    "        for t in reversed(range(n_steps)):\n",
    "            t_batch = torch.ones(shape[0], device=self.device, dtype=torch.long) * t\n",
    "            \n",
    "            # Predict noise in current sample\n",
    "            noise_pred = self.model(x_t, t_batch, condition=condition, context=context)\n",
    "            \n",
    "            # Get diffusion parameters for timestep t\n",
    "            alpha_t = self.alphas[t]\n",
    "            alpha_t_cumprod = self.alphas_cumprod[t]\n",
    "            beta_t = self.betas[t]\n",
    "            \n",
    "            # Add noise only if not the final step\n",
    "            if t > 0:\n",
    "                noise = torch.randn_like(x_t)\n",
    "            else:\n",
    "                noise = 0.\n",
    "            \n",
    "            # Apply reverse diffusion equation\n",
    "            x_t = (1 / torch.sqrt(alpha_t)) * (\n",
    "                x_t - beta_t / torch.sqrt(1 - alpha_t_cumprod) * noise_pred\n",
    "            ) + torch.sqrt(beta_t) * noise\n",
    "        \n",
    "        return x_t\n",
    "\n",
    "\n",
    "class MetricsTracker:\n",
    "    \"\"\"Tracks and computes various metrics during training.\"\"\"\n",
    "    \n",
    "    def __init__(self, device):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            device: Device to run metrics computation on\n",
    "        \"\"\"\n",
    "        self.psnr = PeakSignalNoiseRatio().to(device)\n",
    "        self.ssim = StructuralSimilarityIndexMeasure().to(device)\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Resets all metrics for new epoch.\"\"\"\n",
    "        self.train_losses = []\n",
    "        self.psnr_scores = []\n",
    "        self.ssim_scores = []\n",
    "    \n",
    "    def update(self, pred, target, loss=None):\n",
    "        \"\"\"Updates metrics with new batch results.\"\"\"\n",
    "        if loss is not None:\n",
    "            self.train_losses.append(loss)\n",
    "        if pred is not None and target is not None:\n",
    "            self.psnr_scores.append(self.psnr(pred, target).item())\n",
    "            self.ssim_scores.append(self.ssim(pred, target).item())\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        \"\"\"Returns average metrics for the current period.\"\"\"\n",
    "        return {\n",
    "            'loss': np.mean(self.train_losses) if self.train_losses else 0,\n",
    "            'psnr': np.mean(self.psnr_scores) if self.psnr_scores else 0,\n",
    "            'ssim': np.mean(self.ssim_scores) if self.ssim_scores else 0\n",
    "        }\n",
    "\n",
    "def visualize_samples(t1_real, t2_real, t1_gen, t2_gen, epoch, step, save=True):\n",
    "    \"\"\"Creates visualization grid of real and generated images.\n",
    "    \n",
    "    Args:\n",
    "        t1_real, t2_real: Real T1 and T2 images\n",
    "        t1_gen, t2_gen: Generated T1 and T2 images\n",
    "        epoch (int): Current epoch\n",
    "        step (int): Current step\n",
    "        save (bool): Whether to save the plot\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(15, 4))\n",
    "    \n",
    "    # Real T1\n",
    "    axes[0].imshow(t1_real[0,0].cpu().numpy().T, cmap='gray', origin='lower')\n",
    "    axes[0].set_title('Real T1')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Real T2\n",
    "    axes[1].imshow(t2_real[0,0].cpu().numpy().T, cmap='gray', origin='lower')\n",
    "    axes[1].set_title('Real T2')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Generated T2 from T1\n",
    "    axes[2].imshow(t2_gen[0,0].cpu().numpy().T, cmap='gray', origin='lower')\n",
    "    axes[2].set_title('T1→T2')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    # Generated T1 from T2\n",
    "    axes[3].imshow(t1_gen[0,0].cpu().numpy().T, cmap='gray', origin='lower')\n",
    "    axes[3].set_title('T2→T1')\n",
    "    axes[3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(f'visualizations/diffusion/samples_epoch{epoch}_step{step}.png')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 577 paired T1/T2 datasets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1383e71d44d548d890bda88306085dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/577 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration class containing all training parameters and paths.\"\"\"\n",
    "    def __init__(self):\n",
    "        # Data paths\n",
    "        self.t1_dir = \"../data/IXI_T1\"  # Go up one level from notebooks to root\n",
    "        self.t2_dir = \"../data/IXI_T2\"\n",
    "        \n",
    "        # Model parameters\n",
    "        self.in_channels = 2  # Image + condition channel\n",
    "        self.time_channels = 256\n",
    "        self.n_channels = 64\n",
    "        self.n_timesteps = 1000\n",
    "        self.beta_start = 1e-4\n",
    "        self.beta_end = 0.02\n",
    "        \n",
    "        # Training parameters\n",
    "        self.batch_size = 1\n",
    "    \n",
    "        self.num_epochs = 100\n",
    "        self.lr = 1e-4\n",
    "        self.save_interval = 100  # Save checkpoints every N steps\n",
    "        \n",
    "        # Directories setup - go up one level from notebooks to root\n",
    "        self.checkpoint_dir = Path(\"../checkpoints/diffusion\")\n",
    "        self.vis_dir = Path(\"../visualizations/diffusion\") \n",
    "        self.log_dir = Path(\"../logs/diffusion\")\n",
    "        \n",
    "        # Create directories if they don't exist\n",
    "        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.vis_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Device\n",
    "        self.device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "\n",
    "# Cell 8: Training Functions\n",
    "\n",
    "def load_checkpoint(trainer, checkpoint_path):\n",
    "    \"\"\"Loads model and optimizer state from checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        trainer: DDPMTrainer instance\n",
    "        checkpoint_path: Path to checkpoint file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (epoch, global_step)\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    trainer.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    trainer.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return checkpoint['epoch'], checkpoint['global_step']\n",
    "\n",
    "def save_checkpoint(trainer, epoch, global_step):\n",
    "    \"\"\"Saves model and optimizer state to checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        trainer: DDPMTrainer instance\n",
    "        epoch: Current epoch\n",
    "        global_step: Current global step\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        'model_state_dict': trainer.model.state_dict(),\n",
    "        'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'global_step': global_step,\n",
    "    }\n",
    "    torch.save(checkpoint, \n",
    "               config.checkpoint_dir / f'model_epoch{epoch}_step{global_step}.pt')\n",
    "\n",
    "# Main Training Loop\n",
    "\n",
    "def train_diffusion():\n",
    "    \"\"\"Main training function that handles the complete training pipeline.\"\"\"\n",
    "    \n",
    "    # Initialize tensorboard writer\n",
    "    writer = SummaryWriter(config.log_dir)\n",
    "    \n",
    "    # Initialize dataset and dataloader\n",
    "    dataset = MRIT1T2Dataset(\n",
    "        t1_dir=config.t1_dir,\n",
    "        t2_dir=config.t2_dir,\n",
    "        slice_mode='middle',\n",
    "        paired=True\n",
    "    )\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    # Initialize model and trainer\n",
    "    model = UNet(\n",
    "        in_channels=config.in_channels,\n",
    "        time_channels=config.time_channels,\n",
    "        n_channels=config.n_channels\n",
    "    )\n",
    "    \n",
    "    trainer = DDPMTrainer(\n",
    "        model=model,\n",
    "        n_timesteps=config.n_timesteps,\n",
    "        beta_start=config.beta_start,\n",
    "        beta_end=config.beta_end,\n",
    "        lr=config.lr,\n",
    "        device=config.device\n",
    "    )\n",
    "    \n",
    "    # Initialize metrics tracker\n",
    "    metrics = MetricsTracker(config.device)\n",
    "    \n",
    "    # Check for existing checkpoint\n",
    "    start_epoch = 0\n",
    "    global_step = 0\n",
    "    if config.checkpoint_dir.exists():\n",
    "        checkpoints = list(config.checkpoint_dir.glob('model_epoch*_step*.pt'))\n",
    "        if checkpoints:\n",
    "            latest_checkpoint = max(checkpoints, key=os.path.getctime)\n",
    "            start_epoch, global_step = load_checkpoint(trainer, latest_checkpoint)\n",
    "            print(f\"Resuming from epoch {start_epoch}, step {global_step}\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, config.num_epochs):\n",
    "        metrics.reset()\n",
    "        epoch_pbar = tqdm(dataloader, desc=f\"Epoch {epoch}\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(epoch_pbar):\n",
    "            # Move data to device\n",
    "            t1 = batch['T1'].to(config.device)\n",
    "            t2 = batch['T2'].to(config.device)\n",
    "            \n",
    "            # Train T1 -> T2\n",
    "            loss_t1_t2 = trainer.train_one_batch(\n",
    "                x_0=t2,            # Target is T2\n",
    "                condition=t1,      # Condition on T1\n",
    "                context=t1         # Cross-attention sees T1\n",
    "            )\n",
    "            \n",
    "            # Train T2 -> T1\n",
    "            loss_t2_t1 = trainer.train_one_batch(\n",
    "                x_0=t1,            # Target is T1\n",
    "                condition=t2,      # Condition on T2\n",
    "                context=t2         # Cross-attention sees T2\n",
    "            )\n",
    "            \n",
    "            # Update metrics\n",
    "            avg_loss = (loss_t1_t2 + loss_t2_t1) / 2\n",
    "            metrics.update(None, None, avg_loss)\n",
    "            \n",
    "            # Update progress bar\n",
    "            epoch_pbar.set_postfix({\n",
    "                'loss': f\"{avg_loss:.4f}\",\n",
    "                'step': global_step\n",
    "            })\n",
    "            \n",
    "            # Checkpoint and visualization\n",
    "            if global_step % config.save_interval == 0:\n",
    "                trainer.model.eval()\n",
    "                with torch.no_grad():\n",
    "                    # Generate samples\n",
    "                    t2_gen = trainer.sample(\n",
    "                        condition=t1,\n",
    "                        context=t1,\n",
    "                        shape=t1.shape\n",
    "                    )\n",
    "                    t1_gen = trainer.sample(\n",
    "                        condition=t2,\n",
    "                        context=t2,\n",
    "                        shape=t2.shape\n",
    "                    )\n",
    "                    \n",
    "                    # Calculate metrics for generated images\n",
    "                    metrics.update(t2_gen, t2)\n",
    "                    metrics.update(t1_gen, t1)\n",
    "                    \n",
    "                    # Visualize samples\n",
    "                    visualize_samples(\n",
    "                        t1, t2,\n",
    "                        t1_gen, t2_gen,\n",
    "                        epoch, global_step\n",
    "                    )\n",
    "                    \n",
    "                    # Log to tensorboard\n",
    "                    current_metrics = metrics.get_metrics()\n",
    "                    writer.add_scalar('Loss/train', current_metrics['loss'], global_step)\n",
    "                    writer.add_scalar('Metrics/PSNR', current_metrics['psnr'], global_step)\n",
    "                    writer.add_scalar('Metrics/SSIM', current_metrics['ssim'], global_step)\n",
    "                    writer.add_images('Samples/T1', t1, global_step)\n",
    "                    writer.add_images('Samples/T2', t2, global_step)\n",
    "                    writer.add_images('Samples/T1_generated', t1_gen, global_step)\n",
    "                    writer.add_images('Samples/T2_generated', t2_gen, global_step)\n",
    "                    \n",
    "                    # Save checkpoint\n",
    "                    save_checkpoint(trainer, epoch, global_step)\n",
    "                \n",
    "                trainer.model.train()\n",
    "            \n",
    "            global_step += 1\n",
    "        \n",
    "        # End of epoch\n",
    "        epoch_metrics = metrics.get_metrics()\n",
    "        print(f\"\\nEpoch {epoch} Summary:\")\n",
    "        print(f\"Average Loss: {epoch_metrics['loss']:.4f}\")\n",
    "        print(f\"Average PSNR: {epoch_metrics['psnr']:.2f}\")\n",
    "        print(f\"Average SSIM: {epoch_metrics['ssim']:.4f}\")\n",
    "    \n",
    "    writer.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Start training\n",
    "    train_diffusion()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mri-cyclegan)",
   "language": "python",
   "name": "mri-cyclegan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
